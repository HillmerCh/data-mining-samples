{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('data/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "id": "GQDi3TVJHQjl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and check data"
   ],
   "metadata": {
    "id": "CDJKfeVPHQjn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test_x = pd.read_csv(\"test_x.csv\")\n",
    "train"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "58US-4Z6HQjp",
    "outputId": "caf50387-30fc-454c-aea8-798702ad1265"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess text\n",
    "You may consider applying some preprocessing steps first, such as:\n",
    "- URL data removal;\n",
    "- Turn to lowercase;\n",
    "- Remove hashtags start with @;\n",
    "- Remove special characters (e.g., =!#%@$^&*)\n",
    "- Lemmatization (convert words to base form): https://www.nltk.org/howto/stem.html\n",
    "- ..."
   ],
   "metadata": {
    "id": "s7eQ7o7GHQjq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "#HC Lemmatize\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Example: URL removal\n",
    "def remove_url(data):\n",
    "    re1 = r\"http://\\S+|www\\.\\S+\"\n",
    "    re2 = r\"https://\\S+|www\\.\\S+\"\n",
    "    url_clean= re.compile(\"(%s|%s)\" % (re1, re2))\n",
    "    data=url_clean.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "def to_lowercase(data):\n",
    "    return data.lower()\n",
    "\n",
    "def remove_hashtag(data):\n",
    "    return data.replace(\"@\", \"\")\n",
    "\n",
    "def remove_special(data):\n",
    "    return re.sub('\\W+',' ', data )\n",
    "\n",
    "def lemmatize(data):\n",
    "    tokenized_words=word_tokenize(data)\n",
    "    stemmer=LancasterStemmer()\n",
    "    tokenized_sentence = []\n",
    "    for word in tokenized_words:\n",
    "        tokenized_sentence.append(stemmer.stem(word))\n",
    "\n",
    "    return \" \".join(tokenized_sentence)\n",
    "\n",
    "print('before removal: ', train.iloc[5325]['text'])\n",
    "\n",
    "train['text'] = train['text'].apply(remove_url)\n",
    "test_x['text'] = test_x['text'].apply(remove_url)\n",
    "\n",
    "train['text'] = train['text'].apply(to_lowercase)\n",
    "test_x['text'] = test_x['text'].apply(to_lowercase)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_hashtag)\n",
    "test_x['text'] = test_x['text'].apply(remove_hashtag)\n",
    "\n",
    "\n",
    "train['text'] = train['text'].apply(remove_special)\n",
    "test_x['text'] = test_x['text'].apply(remove_special)\n",
    "\n",
    "train['text'] = train['text'].apply(lemmatize)\n",
    "test_x['text'] = test_x['text'].apply(lemmatize)\n",
    "\n",
    "print('after removal: ', train.iloc[5325]['text'])"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5FnjCG_HQjr",
    "outputId": "71fd302d-6a9b-4f3f-92e8-396bedd753aa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification based on bag-of-word text features\n",
    "\n",
    "\n",
    "Generating hand-crafted bag-of-word feature vectors:\n",
    "- Assign a fixed index to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). Each word essentially is an one-hot embedding (dimension=vocab size);\n",
    "- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary. Each document is a summation of one-hot embedding of all words.\n",
    "- You could filter out words that are rarely or too frequently appeared in documents.\n",
    "\n",
    "sklearn tutorial: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "metadata": {
    "id": "KdMUJECdHQjr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This package generates bag of words feature for each text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# doc for this function: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#vectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=1, max_features=10000)\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=0.001, max_features=10000)#HC\n",
    "#HC max_df = 0.90 \"ignore terms that appear in more than 90% of the documents\".\n",
    "#HC min_df = 0.001 \"ignore terms that appear in less than 0.1% of the documents\".\n",
    "\n",
    "# use training data to construct vocabulary and turn training text into BOW vectors\n",
    "train_bow = vectorizer.fit_transform(train['text'])\n",
    "vocab = vectorizer.vocabulary_.copy()\n",
    "# You can change the vocab size by tuning \"max_df\", \"min_df\" or \"max_features\" in \"CountVectorizer\"\n",
    "print('vocab size: ', len(vocab))\n",
    "print (train_bow.shape) # num_train_text * vocab_size\n",
    "\n",
    "# !!! YOUR TASK 2 (1 point): transform test text into BOW vectors using vectorizer\n",
    "test_bow = vectorizer.transform(test_x['text'])                                      #ME HILLMER\n",
    "print (test_bow.shape) # num_train_text * vocab_size\n"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KG1mmigcHQjs",
    "outputId": "c0072985-7e4b-4149-aec8-73cad515f94b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use basic ML models on BOW features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split given training data into training and validation set\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_bow, train['target'], test_size=0.3, random_state=11)\n",
    "\n",
    "LR = RandomForestClassifier()\n",
    "LR.fit(X_train.toarray(),y_train)\n",
    "print('training recall: ', recall_score(y_train, LR.predict(X_train.toarray())))\n",
    "print('validation recall: ', recall_score(y_val, LR.predict(X_val.toarray())))\n"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QC3hrY8HQjs",
    "outputId": "ae522f15-2dd9-4f9b-dd11-b4f4c733deb2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification based on word2vec embedding features\n",
    "\n",
    "word2vec:\n",
    "- Skip-grams or CBOW\n",
    "- low-dimensional feature vectors with semantic information preserved\n",
    "\n",
    "Before deep learning pervades, gensim is a popular text mining package. Here is a tutorial of gensim's word2vec: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "Now with deep learning, pytorch and tensorflow all has their way to realize word2vec. Tutorial: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ],
   "metadata": {
    "id": "pjG4a4cfHQjt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# train word embeddings using gensim library\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_docs = [train['text'].iloc[i].split(' ') for i in range(len(train['text']))] # gensim only takes list of word list as `sentences`\n",
    "test_x_docs = [test_x['text'].iloc[i].split(' ') for i in range(len(test_x['text']))] # HC\n",
    "\n",
    "w2v_model = Word2Vec(sentences=train_docs, vector_size=100, window=5, negative=20, min_count=2, workers=1)#original\n",
    "w2v_model.train(train_docs, total_examples=len(train_docs), epochs=10)"
   ],
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mkid6vw9HQju",
    "outputId": "09a14ada-0388-48fd-d99a-69c11fc5543e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# check the learned word embeddings\n",
    "\n",
    "# vocabulary\n",
    "w2v_vocav = w2v_model.wv.index_to_key\n",
    "print('example words in vocab: ', [w2v_vocav[i] for i in range(200)])\n",
    "\n",
    "example_word = 'bomb'\n",
    "word_vec = w2v_model.wv[example_word]\n",
    "w2v_model.wv.most_similar(positive=word_vec, topn=10)                             #HC"
   ],
   "metadata": {
    "trusted": true,
    "id": "vf_VO4VcHQju",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "28a729e7-d176-449f-98ac-2050e979aed5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_docs = [train['text'].iloc[i].split(' ') for i in range(len(train['text']))] # gensim only takes list of word list as `sentences`\n",
    "test_x_docs = [test_x['text'].iloc[i].split(' ') for i in range(len(test_x['text']))] # HC\n",
    "\n",
    "w2v_model = Word2Vec(sentences=train_docs, vector_size=1556, window=5, negative=10, min_count=2, workers=1)\n",
    "w2v_model.train(train_docs, total_examples=len(train_docs), epochs=10)\n",
    "\n",
    "# vocabulary\n",
    "w2v_vocav = w2v_model.wv.index_to_key\n",
    "print('example words in vocab: ', [w2v_vocav[i] for i in range(200)])\n",
    "\n",
    "# example embedding\n",
    "example_word = 'bomb'\n",
    "word_vec = w2v_model.wv[example_word]\n",
    "w2v_model.wv.most_similar(positive=word_vec, topn=10)                                                   "
   ],
   "metadata": {
    "id": "NfxFKZJPiXqL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c2dfb1d1-c9d0-4c26-f3a4-627c26ca7091"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use basic ML models on word embeddings\n",
    "\n",
    "def doc2vec(docs, w2v_model):\n",
    "    w2v = []\n",
    "    for doc in docs:\n",
    "        word_embs = [w2v_model.wv[w] for w in doc if w in w2v_model.wv] # get a list of word embeddings\n",
    "\n",
    "        doc_emb = np.mean(np.array(word_embs), axis=0)\n",
    "\n",
    "        # filling zeros for empty doc\n",
    "        if len(word_embs) == 0:\n",
    "            doc_emb = [0 for i in range(w2v_model.vector_size)]\n",
    "\n",
    "        w2v.append(doc_emb)\n",
    "\n",
    "    return np.array(w2v)\n",
    "\n",
    "# transform training tweets into embeddings\n",
    "train_w2v = doc2vec(train_docs, w2v_model)\n",
    "print('training word2vec shape: ', train_w2v.shape)\n",
    "\n",
    "test_w2v = doc2vec(test_x_docs, w2v_model)\n",
    "print('test word2vec shape: ', test_w2v.shape)"
   ],
   "metadata": {
    "trusted": true,
    "id": "sCMPPeyRHQjv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6d7e33cc-8fb7-4372-d655-de8347824d10"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_train_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_docs)]\n",
    "tagged_test_x = [TaggedDocument(doc, [i]) for i, doc in enumerate(test_x)]\n",
    "\n",
    "d2v_model = Doc2Vec(tagged_train_docs, vector_size=1556, window=5, negative=10, min_count=2, workers=20)\n",
    "\n",
    "\n",
    "def doc2vec(docs, d2v_model):\n",
    "    w2v = []\n",
    "    for doc in docs:\n",
    "        w2v.append(d2v_model.infer_vector(doc))\n",
    "    return np.array(w2v)\n",
    "\n",
    "train_d2v = doc2vec(train_docs, d2v_model)\n",
    "print('test doc2vec shape: ', train_d2v.shape)\n",
    "\n",
    "test_d2v = doc2vec(test_x_docs, d2v_model)\n",
    "print('test doc2vec shape: ', test_d2v.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PPvJqISNcL7",
    "outputId": "9e781eae-22ac-416f-c3a9-1a8a58f53f21"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use basic ML models on word2vec features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split given training data into training and validation set\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_w2v, train['target'], test_size=0.3, random_state=11) # Using Word2Vec\n",
    "\n",
    "\n",
    "LR = RandomForestClassifier()\n",
    "LR.fit(X_train,y_train)\n",
    "print('training recall: ', recall_score(y_train, LR.predict(X_train)))\n",
    "print('validation recall: ', recall_score(y_val, LR.predict(X_val)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NALxz8hSRSbx",
    "outputId": "c34ff1c3-b1dd-41a6-cb0c-08cdd81faedf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optional (+1 bonus pt): Classification directly using RNN model\n",
    "RNN model can take in word, turn them into embedding and make predictions\n",
    "\n",
    "Torch tutorial: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ],
   "metadata": {
    "id": "VxyrTfKfHQjv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Prerequisites\n",
    "!pip install 'portalocker>=2.0.0'"
   ],
   "metadata": {
    "id": "SKFztGiNqwy0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train.nunique()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5sZgmv4XLBd",
    "outputId": "1b342aad-bab6-4226-c8eb-460e8d18bf5a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_label(df, column_name):\n",
    "  temp_df = df\n",
    "  le = LabelEncoder()\n",
    "  temp_df['label'] =  le.fit_transform(temp_df[column_name])+1\n",
    "  temp_df = temp_df.drop([column_name], axis=1)\n",
    "  temp_df = temp_df[['label'] + [col for col in temp_df.columns if col != 'label']]\n",
    "  return temp_df\n",
    "\n",
    "train_rnn = encode_label(train[['text', 'keyword']], 'keyword')\n",
    "train_rnn.nunique()\n",
    "\n",
    "train_rnn\n",
    "\n",
    "#train_rnn.groupby(['label']).count()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "ec2BaSTRjcHS",
    "outputId": "63d77718-9f6b-42eb-d432-2058b7585f0e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocess text via tokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = train_rnn['text']#HC\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "print('size of vocab: ', len(vocab))\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ],
   "metadata": {
    "trusted": true,
    "id": "t_1lGCGlHQjw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "582d71e2-706d-45b5-d95f-3db55ae16844"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = list(train_rnn.itertuples(index=False))\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ],
   "metadata": {
    "id": "Bz4KxZvL0nzv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a simple RNN model\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# RNN can be a combination of feature extractor (word2vec) and a classifer\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False) # word2vec\n",
    "        self.fc = nn.Linear(embed_dim, num_class) # classifier\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ],
   "metadata": {
    "id": "5XUjrLBgHQjw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "train_iter = list(train_rnn.itertuples(index=False))\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "print(num_class)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CPFQfmt_Ls0",
    "outputId": "fcb8cb9b-9d2a-423c-fa98-49f2b5c468c5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Define functions to train the RRN model and evaluate results\n",
    "\n",
    "import time\n",
    "\n",
    "def trainRNN(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluateRNN(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n"
   ],
   "metadata": {
    "id": "kwO1OGttvVpp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset and run the model\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "\n",
    "train_iter = list(train_rnn.itertuples(index=False))\n",
    "\n",
    "test_rnn = encode_label(test_x[['text', 'keyword']], 'keyword')\n",
    "test_iter = list(test_rnn.itertuples(index=False)) #HC\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter) #HC\n",
    "test_dataset = to_map_style_dataset(test_iter) #HC\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    trainRNN(train_dataloader)\n",
    "    accu_val = evaluateRNN(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjynQuFjvpO-",
    "outputId": "111cbc9a-d5d6-4ace-bf2c-cb4064355176"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save submission file"
   ],
   "metadata": {
    "id": "kMqgOCgcHQjx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#prediction_y_pred = LR.predict(test_bow.toarray())\n",
    "prediction_y_pred = LR.predict(test_d2v)\n",
    "print(prediction_y_pred)\n",
    "\n",
    "y_pred = pd.DataFrame({\n",
    "    'id': test_x['id'],\n",
    "    'target': prediction_y_pred\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred.to_csv(\"test_submission-vf-2d2v.csv\", index=False)\n",
    "y_pred.head(3)"
   ],
   "metadata": {
    "id": "IwkXV66-HQjy"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
